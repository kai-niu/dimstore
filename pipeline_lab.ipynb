{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SQLContext\n",
    "from pyspark.sql.functions import udf, lit, col, when, avg, countDistinct, year, month\n",
    "from pyspark.sql import Window, DataFrame\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "from pyspark.ml.pipeline import Pipeline, Transformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import dill\n",
    "import codecs\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=SparkContext(appName='jlg')\n",
    "sqlcontext=SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+-------------------+------------------+\n",
      "|                 F1|                 F2|                 F3|                 F4|             Label|\n",
      "+-------------------+-------------------+-------------------+-------------------+------------------+\n",
      "|0.17417849792799078|0.43009799536538307| 0.8422974270679776| 0.6044842735723016| 4.357828105025771|\n",
      "| 0.1585147214988274|0.19787563334191083|0.39240791717944856|  0.504527825099862|2.3978756358240574|\n",
      "|  0.162428980461798| 0.6369621354799332| 0.9080660307106092| 0.9221890530493648| 5.611027617313912|\n",
      "|0.21734977232977237| 0.5961077334846863| 0.3804233377117411| 0.6080774689085126| 4.188054622930284|\n",
      "|0.21017937428222921| 0.6413138500857783|0.15822538788326335|0.10113932772551038|3.4032042523996084|\n",
      "+-------------------+-------------------+-------------------+-------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create dumb pyspark dataframe\n",
    "\n",
    "X1 = np.random.rand(1000).reshape(-1,1)\n",
    "X2 = np.random.rand(1000).reshape(-1,1)\n",
    "X3 = np.random.rand(1000).reshape(-1,1)\n",
    "X4 = np.random.rand(1000).reshape(-1,1)\n",
    "Y = X1*2 + X2*4 + X3*2 + X4*1\n",
    "\n",
    "m = np.hstack([X1,X2,X3,X4,Y])\n",
    "dataset = pd.DataFrame(m)\n",
    "dataset.columns = ['X1','X2','X3','X4','Label']\n",
    "\n",
    "dataset.to_csv('data/foo.csv', index=False)\n",
    "\n",
    "df = sqlcontext.createDataFrame(dataset,schema=[\"F1\", \"F2\", \"F3\", \"F4\", \"Label\"])\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create custome transfomer\n",
    "\n",
    "def Linear_Scaler(params):\n",
    "    \"\"\"\n",
    "    A custom Transformer which scale the value up\n",
    "    \"\"\"\n",
    "    foo = udf(lambda x: x*2)\n",
    "    \n",
    "    context = params['context']\n",
    "    df = context.read.csv('data/foo.csv', header='true', inferSchema = 'true')\n",
    "    \n",
    "    alpha = params['alpha'] \n",
    "    inputCol = 'X1'\n",
    "    outputCol = 'F1'\n",
    "    \n",
    "    # do transform\n",
    "    tmp = df.withColumn(outputCol, df[inputCol]*alpha)\n",
    "    return tmp\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+------------------+------------------+-------------------+\n",
      "|                 X1|                 X2|                 X3|                X4|             Label|                 F1|\n",
      "+-------------------+-------------------+-------------------+------------------+------------------+-------------------+\n",
      "|0.17417849792799078|0.43009799536538307| 0.8422974270679776|0.6044842735723016| 4.357828105025771|0.34835699585598157|\n",
      "| 0.1585147214988274|0.19787563334191083|0.39240791717944856| 0.504527825099862|2.3978756358240574| 0.3170294429976548|\n",
      "|  0.162428980461798| 0.6369621354799332| 0.9080660307106092|0.9221890530493648| 5.611027617313912|  0.324857960923596|\n",
      "+-------------------+-------------------+-------------------+------------------+------------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foo = Linear_Scaler({'context':sqlcontext,'alpha':2.0})\n",
    "foo.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Kai's Feature Store Information ==\n",
      "- Meta Data Manager: default\n",
      "- Supported Meta Data managers:  ['flat file meta manager (default)']\n",
      "- Supported Persistors:  ['flat file persistor (default)']\n",
      "- Supported Serializers:  ['dill Serializer (default)']\n"
     ]
    }
   ],
   "source": [
    "from src.core.store import Store\n",
    "from src.core.feature_meta import Feature_Meta\n",
    "\n",
    "store = Store('store_config.json')\n",
    "f = Feature_Meta('foo_scaler')\n",
    "f.author = 'Kai Niu'\n",
    "f.params = {'context':'The pyspark context, must provided.','alpha':'the scaler coef, optional.'}\n",
    "f.comment = 'scale the data by the coef alpha'\n",
    "\n",
    "store.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "store.register(f, Linear_Scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Feature Catalog ==\n",
      "foo_scaler \t e1c95611-cf7d-4097-9c41-4d564f8b0483 \t 04, Jun 2019 \t Kai Niu\n"
     ]
    }
   ],
   "source": [
    "store.remove('145f33e9-bbaf-443e-ab15-d30e86770e65')\n",
    "store.catalog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Feature Detail ==\n",
      "foo_scaler \t e1c95611-cf7d-4097-9c41-4d564f8b0483 \t 04, Jun 2019 \t Kai Niu\n",
      "params: \n",
      "     context: The pyspark context, must provided.\n",
      "     alpha: the scaler coef, optional.\n",
      "comments: scale the data by the coef alpha\n"
     ]
    }
   ],
   "source": [
    "store.feature_info('e1c95611-cf7d-4097-9c41-4d564f8b0483')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'context':sqlcontext,'alpha':2.0}\n",
    "uid = 'e1c95611-cf7d-4097-9c41-4d564f8b0483'\n",
    "\n",
    "p = store.checkout(uid, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+------------------+------------------+-------------------+\n",
      "|                 X1|                 X2|                 X3|                X4|             Label|                 F1|\n",
      "+-------------------+-------------------+-------------------+------------------+------------------+-------------------+\n",
      "|0.17417849792799078|0.43009799536538307| 0.8422974270679776|0.6044842735723016| 4.357828105025771|0.34835699585598157|\n",
      "| 0.1585147214988274|0.19787563334191083|0.39240791717944856| 0.504527825099862|2.3978756358240574| 0.3170294429976548|\n",
      "|  0.162428980461798| 0.6369621354799332| 0.9080660307106092|0.9221890530493648| 5.611027617313912|  0.324857960923596|\n",
      "+-------------------+-------------------+-------------------+------------------+------------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
